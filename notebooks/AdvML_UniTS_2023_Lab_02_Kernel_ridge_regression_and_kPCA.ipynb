{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Lab 2: *Kernelized* machine learning\n",
    "\n",
    "Advanced Topics in Machine Learning -- Spring 2023, UniTS\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/ganselmif/adv-ml-units/blob/main/notebooks/AdvML_UniTS_2023_Lab_02_Kernel_ridge_regression_and_kPCA.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### *Kernel Regression* and *Kernel Ridge Regression*\n",
    "\n",
    "Recall that the solution of **linear regression** can also be written as: $w=(X^{T}X)^{-1}X^{T}y=X^{T}(XX^{T})^{-1}y$\n",
    "\n",
    "Let $X\\in R^{N\\times d}$: we have $X^{T}X\\in R^{d\\times d}$ and $K=XX^{T}\\in R^{N\\times N}$. Whether it is more convenient to (pre)compute which matrix product (among $X^{T}X$, $XX^{T}$) depends on the $d/N$ ratio.\n",
    "\n",
    "As far as predictions are concerned, we have that: $f(z)=z^{T}w=z^{T} X^{T}(XX^{T})^{-1}y= \\alpha^{T}(z)K^{-1}y$, with $\\alpha(z)=z^{T}X^{T}=K(z,X)\\in R^{1\\times N}$.\n",
    "\n",
    "How can we move to the non-linear regression case?  We just substitute $x\\rightarrow \\phi(x)$, and the reasoning above can be repeated!\n",
    "\n",
    "For more information, you can look up [this Medium article](https://knork.medium.com/linear-regression-in-python-from-scratch-with-kernels-e9c37f7975b9) or [this code-first tutorial](https://github.com/luigicarratino/Tutorial_Kernels_MLSS2019_London/blob/master/Tutorial%20Kernel.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import make_moons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Exercise 1: linear data regression**\n",
    "1. Generate and plot a dataset consisting in $100$ datapoints in the form $(x_i,y_i)$. The $x_i$ are sampled uniformly in $[2,30]$, whereas the $y_i$ are sampled from a Gaussian distribution centred in $2x_i + 50$ having unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "2. Fit a linear regression model to the data, **with no learnable intercept** (i.e. fix it to zero), and plot the predictions of the resulting model compared to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "3. Fit a linear regression model to the data, **with learnable intercept**, and plot the predictions of the resulting model compared to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "4. Fit a linear ridge regression model to the data, and plot the predictions of the resulting model compared to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# (uso la def che mi viene dal teorema di repr con K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Exercise 2: Kernel regression on *periodic* data**\n",
    "1. The following dataset is given. Plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xkr = np.linspace(2, 30, 100)\n",
    "ykr = xkr + 4 * np.sin(xkr) + 4 * np.random.rand(xkr.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "2. Define a function that computes the Gaussian kernel value between two vectors, represented as numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gaussian_kernel(x1, x2, sigma):\n",
    "    # YOUR CODE HERE\n",
    "    # (uso esercizio del foglio pecedente)\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "3. With the fuction just defined, compute the Gram matrix of the dataset. Use a Gaussian kernel with $\\sigma=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "4. Fit a kernel ridge regression model to the data, and plot the resulting model compared to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# ( di nuoo uso repr th)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### *Kernel PCA* and the *Radial Basis Function* (RBF) Kernel\n",
    "\n",
    "Let us briefly recap the key concepts of PCA. We have a dataset $X\\in R^{N\\times d}$, and we want to find a new basis $Z\\in R^{N\\times d}$ such that the variance of the projected data is maximized. This is equivalent to finding the eigenvectors of the covariance matrix $C=X^{T}X$.\n",
    "\n",
    "This requires the data to be centered, i.e. $X_{i}^{(j)}-\\mu_{j} \\rightarrow X_{i}^{(j)}$, where $\\mu_{j}=\\frac{1}{N}\\sum_{i=1}^{N}X_{i}^{(j)}$ is the mean of the $j$-th feature.\n",
    "\n",
    "We can also write the eigenvalue problem in components, as follows: $Cz_{j}=\\lambda_{j}z_{j}$, where $z_{j}$ is the $j$-th eigenvector and $\\lambda_{j}$ is the corresponding eigenvalue. The eigenvectors are orthogonal, i.e. $z_{j}^{T}z_{k}=0$ for $j\\neq k$.\n",
    "\n",
    "Moving to the kernelized case, we operate the substitution $x\\rightarrow \\phi(x)$, and the eigenvalue problem becomes $C_{K}v=\\lambda v$ with $C_{K}=\\frac{1}{N}\\sum_{i}\\phi(x_{i})\\phi^{T}(x_{i})$, $\\phi(\\cdot)$ being a generic feature map.\n",
    "\n",
    "One can prove that solutions of the eigenvalue problem are in the form $v=\\sum_{i}\\alpha_{i}\\phi(x_{i})$. Multiplying both sides of $C_{K}v=\\lambda v$ by $\\phi(x_{k})$ and substituting, we obtain $N\\lambda \\alpha=K \\alpha$.\n",
    "\n",
    "Such reasoning still requires normalization, i.e. $ \\phi(x)-\\frac{1}{d}\\sum_{i}(\\phi(x))_{i} \\rightarrow \\phi(x)$. The resulting kernel $K$ is in the form $K := K-2Id_{1/n}K + Id_{1/n}K Id_{1/n}$ with $Id_{1/n}$ the matrix with entries $1/n$.\n",
    "\n",
    "A more thorough explanation can be found in [this blogpost](https://sdat.ir/en/sdat-blog/python-kernel-tricks-and-nonlinear-dimensionality-reduction-via-rbf-kernel-pca).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Exercise 3: Kernel PCA**\n",
    "1. A *Half Moons* dataset of $100$ points is given below. Plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xhm, yhm = make_moons(n_samples=100, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "2. Apply the PCA algorithm to the dataset, and plot the resulting projection using the first 2 principal components.\n",
    "**Hint**: you can use the `PCA` class from `sklearn.decomposition`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "3. Repeat the previous step, but plotting just the first principal component. Comment on the separability of the two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "4. Implement a function that computes the *Radial Basis* PCA algorithm of the dataset, given as a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def kpca(x_data, gamma, n_components):\n",
    "    \"\"\"\n",
    "    Implementation of a RBF kernel PCA.\n",
    "\n",
    "    Arguments:\n",
    "        x_data: A MxN dataset as NumPy array where the samples are stored as rows (M),\n",
    "           and the attributes defined as columns (N).\n",
    "        gamma: The free parameter (coefficient) for the RBF kernel.\n",
    "        n_components: The number of components to be returned.\n",
    "\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    xpcs = ...\n",
    "    return xpcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "5. Apply the function just defined to the dataset, and plot the resulting projection using the first 2 principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "6. Repeat the previous step, but plotting just the first principal component. Comment on the separability of the two classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPkibvpTEMRILBn2/x8IuJj",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
